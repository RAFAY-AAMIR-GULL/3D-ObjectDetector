{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install opencv-python\n",
    "!pip3 install matplotlib\n",
    "\n",
    "# apt-get update && apt-get install ffmpeg libsm6 libxext6 -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import argparse\n",
    "from models.retinanet import build_retinanet\n",
    "from modules import utils\n",
    "from torchvision import transforms\n",
    "import data.transforms as vtf\n",
    "from data import VideoDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2bool(v):\n",
    "    return v.lower() in (\"yes\", \"true\", \"t\", \"1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--MULTI_GPUS'], dest='MULTI_GPUS', nargs=None, const=None, default=True, type=<function str2bool at 0x75cdaea7ac10>, choices=None, help='If  more than 0 then use all visible GPUs by default only one GPU used ', metavar=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(\n",
    "    description=\"Training single stage FPN with OHEM, resnet as backbone\"\n",
    ")\n",
    "\n",
    "parser.add_argument(\"--DATA_ROOT\", type=str, default=\"/workspace/\")\n",
    "parser.add_argument(\"--SAVE_ROOT\", type=str, default='/workspace/road/cache/resnet50I3D512-Pkinetics-b4s8x1x1-roadt3-h3x3x3/')\n",
    "parser.add_argument(\"--MODEL_PATH\", type=str, default='/workspace/kinetics-pt/')\n",
    "\n",
    "parser.add_argument(\"--ANNO_ROOT\", type=str, default=\"\")\n",
    "parser.add_argument(\n",
    "    \"--MODE\",\n",
    "    default=\"gen_dets\",\n",
    "    help=\"MODE can be train, gen_dets, eval_frames, eval_tubes define SUBSETS accordingly, build tubes\",\n",
    ")\n",
    "# Name of backbone network, e.g. resnet18, resnet34, resnet50, resnet101 resnet152 are supported\n",
    "parser.add_argument(\"--ARCH\", default=\"resnet50\", type=str, help=\" base arch\")\n",
    "parser.add_argument(\"--MODEL_TYPE\", default=\"I3D\", type=str, help=\" base model\")\n",
    "parser.add_argument(\"--model_subtype\", default=\"I3D\", type=str, help=\" sub model\")\n",
    "parser.add_argument(\n",
    "    \"--ANCHOR_TYPE\",\n",
    "    default=\"RETINA\",\n",
    "    type=str,\n",
    "    help=\"type of anchors to be used in model\",\n",
    ")\n",
    "\n",
    "parser.add_argument(\"--SEQ_LEN\", default=8, type=int, help=\"NUmber of input frames\")\n",
    "parser.add_argument(\n",
    "    \"--TEST_SEQ_LEN\", default=8, type=int, help=\"NUmber of input frames\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--MIN_SEQ_STEP\",\n",
    "    default=1,\n",
    "    type=int,\n",
    "    help=\"DIFFERENCE of gap between the frames of sequence\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--MAX_SEQ_STEP\",\n",
    "    default=1,\n",
    "    type=int,\n",
    "    help=\"DIFFERENCE of gap between the frames of sequence\",\n",
    ")\n",
    "# if output heads are have shared features or not: 0 is no-shareing else sharining enabled\n",
    "# parser.add_argument('--MULIT_SCALE', default=False, type=str2bool,help='perfrom multiscale training')\n",
    "parser.add_argument(\n",
    "    \"--HEAD_LAYERS\",\n",
    "    default=3,\n",
    "    type=int,\n",
    "    help=\"0 mean no shareding more than 0 means shareing\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--NUM_FEATURE_MAPS\",\n",
    "    default=5,\n",
    "    type=int,\n",
    "    help=\"0 mean no shareding more than 0 means shareing\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--CLS_HEAD_TIME_SIZE\",\n",
    "    default=3,\n",
    "    type=int,\n",
    "    help=\"Temporal kernel size of classification head\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--REG_HEAD_TIME_SIZE\",\n",
    "    default=3,\n",
    "    type=int,\n",
    "    help=\"Temporal kernel size of regression head\",\n",
    ")\n",
    "\n",
    "#  Name of the dataset only voc or coco are supported\n",
    "parser.add_argument(\n",
    "    \"--DATASET\", default=\"road\", type=str, help=\"dataset being used\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--TRAIN_SUBSETS\",\n",
    "    default=\"train_3,\",\n",
    "    type=str,\n",
    "    help=\"Training SUBSETS seprated by ,\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--VAL_SUBSETS\", default=\"\", type=str, help=\"Validation SUBSETS seprated by ,\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--TEST_SUBSETS\", default=\"\", type=str, help=\"Testing SUBSETS seprated by ,\"\n",
    ")\n",
    "# Input size of image only 600 is supprted at the moment\n",
    "parser.add_argument(\"--MIN_SIZE\", default=512, type=int, help=\"Input Size for FPN\")\n",
    "\n",
    "#  data loading argumnets\n",
    "parser.add_argument(\n",
    "    \"-b\", \"--BATCH_SIZE\", default=4, type=int, help=\"Batch size for training\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--TEST_BATCH_SIZE\", default=1, type=int, help=\"Batch size for testing\"\n",
    ")\n",
    "# Number of worker to load data in parllel\n",
    "parser.add_argument(\n",
    "    \"--NUM_WORKERS\",\n",
    "    \"-j\",\n",
    "    default=8,\n",
    "    type=int,\n",
    "    help=\"Number of workers used in dataloading\",\n",
    ")\n",
    "# optimiser hyperparameters\n",
    "parser.add_argument(\"--OPTIM\", default=\"SGD\", type=str, help=\"Optimiser type\")\n",
    "parser.add_argument(\"--RESUME\", default=0, type=int, help=\"Resume from given epoch\")\n",
    "parser.add_argument(\n",
    "    \"--MAX_EPOCHS\", default=30, type=int, help=\"Number of training epoc\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"-l\",\n",
    "    \"--LR\",\n",
    "    \"--learning-rate\",\n",
    "    default=0.004225,\n",
    "    type=float,\n",
    "    help=\"initial learning rate\",\n",
    ")\n",
    "parser.add_argument(\"--MOMENTUM\", default=0.9, type=float, help=\"momentum\")\n",
    "parser.add_argument(\n",
    "    \"--MILESTONES\", default=\"20,25\", type=str, help=\"Chnage the lr @\"\n",
    ")\n",
    "parser.add_argument(\"--GAMMA\", default=0.1, type=float, help=\"Gamma update for SGD\")\n",
    "parser.add_argument(\n",
    "    \"--WEIGHT_DECAY\", default=1e-4, type=float, help=\"Weight decay for SGD\"\n",
    ")\n",
    "\n",
    "# Freeze layers or not\n",
    "parser.add_argument(\n",
    "    \"--FBN\",\n",
    "    \"--FREEZE_BN\",\n",
    "    default=True,\n",
    "    type=str2bool,\n",
    "    help=\"freeze bn layers if true or else keep updating bn layers\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--FREEZE_UPTO\",\n",
    "    default=1,\n",
    "    type=int,\n",
    "    help=\"layer group number in ResNet up to which needs to be frozen\",\n",
    ")\n",
    "\n",
    "# Loss function matching threshold\n",
    "parser.add_argument(\n",
    "    \"--POSTIVE_THRESHOLD\",\n",
    "    default=0.5,\n",
    "    type=float,\n",
    "    help=\"Min threshold for Jaccard index for matching\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--NEGTIVE_THRESHOLD\",\n",
    "    default=0.4,\n",
    "    type=float,\n",
    "    help=\"Max threshold Jaccard index for matching\",\n",
    ")\n",
    "# Evaluation hyperparameters\n",
    "parser.add_argument(\n",
    "    \"--EVAL_EPOCHS\",\n",
    "    default=\"30\",\n",
    "    type=str,\n",
    "    help=\"eval epochs to test network on these epoch checkpoints usually the last epoch is used\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--VAL_STEP\",\n",
    "    default=1,\n",
    "    type=int,\n",
    "    help=\"Number of training epoch before evaluation\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--IOU_THRESH\",\n",
    "    default=0.5,\n",
    "    type=float,\n",
    "    help=\"Evaluation threshold for validation and for frame-wise mAP\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--CONF_THRESH\",\n",
    "    default=0.025,\n",
    "    type=float,\n",
    "    help=\"Confidence threshold for to remove detection below given number\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--NMS_THRESH\",\n",
    "    default=0.5,\n",
    "    type=float,\n",
    "    help=\"NMS threshold to apply nms at the time of validation\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--TOPK\", default=10, type=int, help=\"topk detection to keep for evaluation\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--GEN_CONF_THRESH\",\n",
    "    default=0.05,\n",
    "    type=float,\n",
    "    help=\"Confidence threshold at the time of generation and dumping\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--GEN_TOPK\", default=100, type=int, help=\"topk at the time of generation\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--GEN_NMS\", default=0.5, type=float, help=\"NMS at the time of generation\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--CLASSWISE_NMS\",\n",
    "    default=False,\n",
    "    type=str2bool,\n",
    "    help=\"apply classwise NMS/no tested properly\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--JOINT_4M_MARGINALS\",\n",
    "    default=False,\n",
    "    type=str2bool,\n",
    "    help=\"generate score of joints i.e. duplexes or triplet by marginals like agents and actions scores\",\n",
    ")\n",
    "\n",
    "## paths hyper parameters\n",
    "parser.add_argument(\n",
    "    \"--COMPUTE_PATHS\",\n",
    "    default=False,\n",
    "    type=str2bool,\n",
    "    help=\" COMPUTE_PATHS if set true then it overwrite existing ones\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--PATHS_IOUTH\",\n",
    "    default=0.5,\n",
    "    type=float,\n",
    "    help=\"Iou threshold for building paths to limit neighborhood search\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--PATHS_COST_TYPE\",\n",
    "    default=\"score\",\n",
    "    type=str,\n",
    "    help=\"cost function type to use for matching, other options are scoreiou, iou\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--PATHS_JUMP_GAP\",\n",
    "    default=4,\n",
    "    type=int,\n",
    "    help=\"GAP allowed for a tube to be kept alive after no matching detection found\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--PATHS_MIN_LEN\", default=6, type=int, help=\"minimum length of generated path\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--PATHS_MINSCORE\",\n",
    "    default=0.1,\n",
    "    type=float,\n",
    "    help=\"minimum score a path should have over its length\",\n",
    ")\n",
    "\n",
    "## paths hyper parameters\n",
    "parser.add_argument(\n",
    "    \"--COMPUTE_TUBES\",\n",
    "    default=False,\n",
    "    type=str2bool,\n",
    "    help=\"if set true then it overwrite existing tubes\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--TUBES_ALPHA\",\n",
    "    default=0,\n",
    "    type=float,\n",
    "    help=\"alpha cost for changeing the label\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--TRIM_METHOD\",\n",
    "    default=\"none\",\n",
    "    type=str,\n",
    "    help=\"other one is indiv which works for UCF24\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--TUBES_TOPK\",\n",
    "    default=10,\n",
    "    type=int,\n",
    "    help=\"Number of labels to assign for a tube\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--TUBES_MINLEN\", default=5, type=int, help=\"minimum length of a tube\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--TUBES_EVAL_THRESHS\",\n",
    "    default=\"0.2,0.5\",\n",
    "    type=str,\n",
    "    help=\"evaluation threshold for checking tube overlap at evaluation time, one can provide as many as one wants\",\n",
    ")\n",
    "\n",
    "###\n",
    "parser.add_argument(\n",
    "    \"--LOG_START\",\n",
    "    default=10,\n",
    "    type=int,\n",
    "    help=\"start loging after k steps for text/tensorboard\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--LOG_STEP\",\n",
    "    default=10,\n",
    "    type=int,\n",
    "    help=\"Log every k steps for text/tensorboard\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--TENSORBOARD\",\n",
    "    default=1,\n",
    "    type=str2bool,\n",
    "    help=\"Use tensorboard for loss/evalaution visualization\",\n",
    ")\n",
    "\n",
    "# Program arguments\n",
    "parser.add_argument(\n",
    "    \"--MAN_SEED\", default=123, type=int, help=\"manualseed for reproduction\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--MULTI_GPUS\",\n",
    "    default=True,\n",
    "    type=str2bool,\n",
    "    help=\"If  more than 0 then use all visible GPUs by default only one GPU used \",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your working directories are::\n",
      "LOAD::>  /workspace/ \n",
      "SAVE::>  /workspace/road/cache/resnet50I3D512-Pkinetics-b4s8x1x1-roadt3-h3x3x3/\n",
      "Your model will be initialized using /workspace/kinetics-pt/resnet50I3D.pth\n"
     ]
    }
   ],
   "source": [
    "# Use CUDA_VISIBLE_DEVICES=0,1,4,6 to select GPUs to use\n",
    "\n",
    "## Parse arguments\n",
    "# args = utils.parse_my_args(parser_module=parser)\n",
    "args, unknown = parser.parse_known_args()\n",
    "args = utils.set_args(args)  # set directories and SUBSETS fo datasets\n",
    "args.MULTI_GPUS = False if args.BATCH_SIZE == 1 else args.MULTI_GPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "database.keys() dict_keys(['2014-06-25-16-45-34_stereo_centre_02', '2014-06-26-09-53-12_stereo_centre_02', '2014-07-14-14-49-50_stereo_centre_01', '2014-07-14-15-42-55_stereo_centre_03', '2014-08-08-13-15-11_stereo_centre_01', '2014-08-11-10-59-18_stereo_centre_02', '2014-11-14-16-34-33_stereo_centre_06', '2014-11-18-13-20-12_stereo_centre_05', '2014-11-21-16-07-03_stereo_centre_01', '2014-11-25-09-18-32_stereo_centre_04', '2014-12-09-13-21-02_stereo_centre_01', '2015-02-03-08-45-10_stereo_centre_02', '2015-02-03-19-43-11_stereo_centre_04', '2015-02-06-13-57-16_stereo_centre_02', '2015-02-13-09-16-26_stereo_centre_02', '2015-02-13-09-16-26_stereo_centre_05', '2015-02-24-12-32-19_stereo_centre_04', '2015-03-03-11-31-36_stereo_centre_01'])\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if args.MODE == \"train\":\n",
    "    args.TEST_SEQ_LEN = args.SEQ_LEN\n",
    "else:\n",
    "    args.SEQ_LEN = args.TEST_SEQ_LEN\n",
    "\n",
    "args.SEQ_LEN = args.TEST_SEQ_LEN\n",
    "args.MAX_SEQ_STEP = 1\n",
    "args.SUBSETS = args.TEST_SUBSETS\n",
    "full_test = True  # args.MODE != 'train'\n",
    "args.skip_beggning = 0\n",
    "args.skip_ending = 0\n",
    "if args.MODEL_TYPE == \"I3D\" or \"SlowFast\":\n",
    "    args.skip_beggning = 2\n",
    "    args.skip_ending = 2\n",
    "elif args.MODEL_TYPE != \"C2D\":\n",
    "    args.skip_beggning = 2\n",
    "\n",
    "skip_step = args.SEQ_LEN - args.skip_beggning\n",
    "\n",
    "val_transform = transforms.Compose(\n",
    "    [\n",
    "        vtf.ResizeClip(args.MIN_SIZE, args.MAX_SIZE),\n",
    "        vtf.ToTensorStack(),\n",
    "        vtf.Normalize(mean=args.MEANS, std=args.STDS),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_dataset = VideoDataset(\n",
    "    args,\n",
    "    train=False,\n",
    "    transform=val_transform,\n",
    "    skip_step=skip_step,\n",
    "    full_test=full_test,\n",
    ")\n",
    "\n",
    "args.num_classes = val_dataset.num_classes\n",
    "# one for objectness\n",
    "args.label_types = val_dataset.label_types\n",
    "args.num_label_types = val_dataset.num_label_types\n",
    "args.all_classes = val_dataset.all_classes\n",
    "args.num_classes_list = val_dataset.num_classes_list\n",
    "args.num_ego_classes = val_dataset.num_ego_classes\n",
    "args.ego_classes = val_dataset.ego_classes\n",
    "args.head_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = build_retinanet(args).cuda()\n",
    "if args.MULTI_GPUS:\n",
    "    # logger.info(\"\\nLets do dataparallel\\n\")\n",
    "    net = torch.nn.DataParallel(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_757/2061456033.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(torch.load(args.MODEL_PATH))\n"
     ]
    }
   ],
   "source": [
    "for epoch in args.EVAL_EPOCHS:\n",
    "    args.MODEL_PATH = args.SAVE_ROOT + \"model_{:06d}.pth\".format(epoch)\n",
    "    net.eval()\n",
    "    net.load_state_dict(torch.load(args.MODEL_PATH))\n",
    "    net.eval()  # switch net to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/workspace/road/rgb-images/2014-12-09-13-21-02_stereo_centre_01/00000.jpg', '/workspace/road/rgb-images/2014-12-09-13-21-02_stereo_centre_01/00001.jpg', '/workspace/road/rgb-images/2014-12-09-13-21-02_stereo_centre_01/00002.jpg', '/workspace/road/rgb-images/2014-12-09-13-21-02_stereo_centre_01/00003.jpg', '/workspace/road/rgb-images/2014-12-09-13-21-02_stereo_centre_01/00004.jpg', '/workspace/road/rgb-images/2014-12-09-13-21-02_stereo_centre_01/00005.jpg', '/workspace/road/rgb-images/2014-12-09-13-21-02_stereo_centre_01/00006.jpg', '/workspace/road/rgb-images/2014-12-09-13-21-02_stereo_centre_01/00007.jpg']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_757/3189238623.py:9: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  images = torch.tensor([images]).permute(0, 4, 1, 2, 3).cuda(0, non_blocking=True)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "base_path = \"/workspace/road/rgb-images\"\n",
    "event = \"2014-12-09-13-21-02_stereo_centre_01\"\n",
    "im_list = [\"00000.jpg\", \"00001.jpg\", \"00002.jpg\", \"00003.jpg\", \"00004.jpg\", \"00005.jpg\", \"00006.jpg\", \"00007.jpg\"]\n",
    "imgs_paths = [os.path.join(base_path, event, im) for im in im_list]\n",
    "print(imgs_paths)\n",
    "images = [cv2.resize(cv2.imread(img_path).astype('float32'), (704, 512)) for img_path in imgs_paths]\n",
    "images = torch.tensor([images]).permute(0, 4, 1, 2, 3).cuda(0, non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # torch.cuda.synchronize()\n",
    "# # Create a tensor with the given size\n",
    "# images = torch.randn(1, 3, 8, 512, 704).cuda(0, non_blocking=True)\n",
    "\n",
    "# # Use permute to change the dimensions\n",
    "# tensor_permuted = images.permute(0, 2, 1, 3, 4)\n",
    "\n",
    "# # Verify the shape of the resulting tensor\n",
    "# print(images.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.synchronize()\n",
    "activation = torch.nn.Sigmoid().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "decoded_boxes, confidences, ego_preds = net(images)\n",
    "confidence = activation(confidences)\n",
    "seq_len = ego_preds.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 67536, 4])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_boxes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 67536, 149])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confidences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 67536, 149])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confidence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_batch = confidence[0, 0]\n",
    "scores = confidence_batch[:, 0].squeeze().clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_boxes_batch = decoded_boxes[0,0]\n",
    "confidence_batch = confidence[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confidence_batch shape torch.Size([67536, 149])\n",
      "decoded_boxes_batch shape torch.Size([67536, 4])\n",
      "scores shape torch.Size([67536])\n"
     ]
    }
   ],
   "source": [
    "print(\"confidence_batch shape\", confidence_batch.shape)\n",
    "print(\"decoded_boxes_batch shape\", decoded_boxes_batch.shape)\n",
    "print(\"scores shape\", scores.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_dets, save_data = utils.filter_detections_for_dumping(\n",
    "    args, scores, decoded_boxes_batch, confidence_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 153)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
